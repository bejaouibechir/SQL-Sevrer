# Execices Jobs, Schedules, Alertes, Proxy, Operators
Voici une nouvelle version des **20 exercices** pour le thème **Jobs, Schedules, Alertes, Proxy, Operators**, prenant en compte vos remarques. Chaque exercice inclut une configuration de **Papercut SMTP**, un test avec **Database Mail**, et des exemples **ETL réalistes et intuitifs** avec plusieurs étapes d’extraction, transformation, et chargement.

---

### **Préambule : Installation et configuration de Papercut SMTP**

---

#### **Installation de Papercut SMTP** :
1. **Téléchargez Papercut SMTP** depuis [ce lien GitHub](https://github.com/ChangemakerStudios/Papercut-SMTP/releases).
2. Décompressez le fichier téléchargé et lancez `Papercut.SMTP.exe`.
3. L’interface de Papercut SMTP doit s’afficher, indiquant qu’il est prêt à capturer les e-mails sur le port **25**.

---

#### **Configuration dans SQL Server** :
1. Activez **Database Mail** dans SQL Server :
   ```sql
   EXEC sp_configure 'show advanced options', 1;
   RECONFIGURE;
   EXEC sp_configure 'Database Mail XPs', 1;
   RECONFIGURE;
   ```
2. Dans **SSMS**, configurez un compte Database Mail avec les paramètres suivants :
   - **Account name** : PapercutAccount
   - **E-mail address** : `me780411@gmail.com`
   - **Display name** : Papercut SMTP
   - **Reply e-mail** : `me780411@gmail.com`
   - **Server name** : `localhost`
   - **Port** : `25`
   - **Authentication** : Anonyme
3. Créez un profil nommé `PapercutProfile` et testez avec :
   ```sql
   EXEC msdb.dbo.sp_send_dbmail
       @profile_name = 'PapercutProfile',
       @recipients = 'me780411@gmail.com',
       @subject = 'Test Email from Database Mail',
       @body = 'This is a test email sent using Papercut SMTP.';
   ```

Vérifiez l'e-mail dans l'interface Papercut.

---

### **Exercices 1 à 5 : Jobs ETL (Graphique)**

---

#### **Exercice 1 : Sauvegarder des données dans un fichier JSON**
- **Objectif** : Extraire des données de `Sales.SalesOrderHeader` et les sauvegarder dans un fichier JSON.
- **Étapes** :
  1. Créez un Job nommé **BackupSalesData**.
  2. **Étape 1 : Extraction des données** :
     - **Commande T-SQL** :
       ```sql
       SELECT * 
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       WHERE OrderDate >= DATEADD(YEAR, -1, GETDATE());
       ```
  3. **Étape 2 : Sauvegarde des données dans un fichier JSON** :
     - **Commande T-SQL** :
       ```sql
       SELECT *
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       FOR JSON AUTO, ROOT('SalesOrders')
       INTO OUTFILE 'C:\Exports\SalesOrders.json';
       ```

---

#### **Exercice 2 : Fusionner deux tables et sauvegarder dans une nouvelle table**
- **Objectif** : Faire une jointure entre `Sales.SalesOrderHeader` et `Sales.SalesOrderDetail`, puis charger les données dans une nouvelle table.
- **Étapes** :
  1. Créez un Job nommé **MergeSalesData**.
  2. **Étape 1 : Créer la table de destination** :
     - **Commande T-SQL** :
       ```sql
       CREATE TABLE SalesMerged (
           OrderID INT,
           OrderDate DATE,
           ProductID INT,
           Quantity INT,
           TotalDue DECIMAL(18, 2)
       );
       ```
  3. **Étape 2 : Faire la jointure et charger les données** :
     - **Commande T-SQL** :
       ```sql
       INSERT INTO SalesMerged (OrderID, OrderDate, ProductID, Quantity, TotalDue)
       SELECT soh.SalesOrderID, soh.OrderDate, sod.ProductID, sod.OrderQty, soh.TotalDue
       FROM AdventureWorks2022.Sales.SalesOrderHeader soh
       JOIN AdventureWorks2022.Sales.SalesOrderDetail sod
       ON soh.SalesOrderID = sod.SalesOrderID;
       ```

---

#### **Exercice 3 : Extraire, trier et sauvegarder dans une base spécifique**
- **Objectif** : Extraire les données de `SalesOrderHeader`, trier par montant décroissant et sauvegarder dans une base secondaire.
- **Étapes** :
  1. Créez une base secondaire `ETL_Destination` :
     ```sql
     CREATE DATABASE ETL_Destination;
     ```
  2. Créez un Job nommé **SortedSalesETL**.
  3. **Étape 1 : Extraction des données** :
     - **Commande T-SQL** :
       ```sql
       SELECT * 
       INTO TempSortedSales
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       ORDER BY TotalDue DESC;
       ```
  4. **Étape 2 : Charger dans `ETL_Destination`** :
     - **Commande T-SQL** :
       ```sql
       USE ETL_Destination;
       SELECT * INTO SortedSales FROM TempSortedSales;
       ```

---

#### **Exercice 4 : Filtrer et exporter des données dans un fichier CSV**
- **Objectif** : Filtrer les commandes supérieures à 5000$ et exporter dans un fichier CSV.
- **Étapes** :
  1. Créez un Job nommé **ExportHighValueOrders**.
  2. **Étape 1 : Filtrer les données** :
     - **Commande T-SQL** :
       ```sql
       SELECT * INTO HighValueOrders
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       WHERE TotalDue > 5000;
       ```
  3. **Étape 2 : Exporter en CSV** :
     - **Commande PowerShell simple** :
       ```powershell
       bcp "SELECT * FROM AdventureWorks2022.dbo.HighValueOrders" queryout "C:\Exports\HighValueOrders.csv" -c -T
       ```

---

#### **Exercice 5 : Créer une table agrégée des ventes annuelles**
- **Objectif** : Agréger les ventes par année et sauvegarder dans une nouvelle table.
- **Étapes** :
  1. Créez un Job nommé **AggregateSalesByYear**.
  2. **Étape 1 : Créer la table agrégée** :
     - **Commande T-SQL** :
       ```sql
       CREATE TABLE YearlySales (
           Year INT,
           TotalSales DECIMAL(18, 2)
       );
       ```
  3. **Étape 2 : Charger les données agrégées** :
     - **Commande T-SQL** :
       ```sql
       INSERT INTO YearlySales (Year, TotalSales)
       SELECT YEAR(OrderDate), SUM(TotalDue)
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       GROUP BY YEAR(OrderDate);
       ```

---

### **Exercices 6 à 10 : Introduction aux Schedules et Database Mail**

---

#### **Exercice 6 : Planifier un Job pour exécuter tous les jours**
- **Objectif** : Automatiser un job pour qu’il s’exécute quotidiennement à 1h.
- **Instructions** :
  1. Créez un Schedule nommé `DailySchedule`.
  2. Associez le Job **AggregateSalesByYear** au Schedule.

---

#### **Exercice 7 : Configurer une alerte en cas d’échec**
- **Objectif** : Recevoir un e-mail en cas d’échec.
- **Instructions** :
  1. Associez une alerte à un **Operator** (`AdminOperator`) pour notifier en cas d’échec.

---

Les **exercices 11 à 20** combineront des Schedules, Proxy, et Operators avec des scénarios avancés. Voulez-vous que je les détaille ?

### **Exercices 11 à 20 : Scénarios avancés avec Schedules, Proxy, Alertes, et Operators**

### **Exercice 11 : Job avec Schedule pour surveiller les ventes mensuelles**
- **Objectif** : Planifier un Job pour extraire les ventes mensuelles, les agréger, et charger les résultats dans une table dédiée.
- **Étapes** :
  1. Créez un Job nommé **MonthlySalesMonitor**.
  2. **Étape 1 : Extraire les ventes mensuelles** :
     - **Commande T-SQL** :
       ```sql
       SELECT * INTO MonthlySales
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       WHERE OrderDate >= DATEADD(MONTH, -1, GETDATE());
       ```
  3. **Étape 2 : Agréger les données** :
     - **Commande T-SQL** :
       ```sql
       SELECT COUNT(*) AS OrderCount, SUM(TotalDue) AS TotalSales
       INTO MonthlyAggregatedSales
       FROM MonthlySales;
       ```
  4. Ajoutez un Schedule pour exécuter ce Job le 1er de chaque mois à 2h00.

---

### **Exercice 12 : Créer une alerte pour les ventes exceptionnelles**
- **Objectif** : Surveiller les commandes dépassant 50 000$ et notifier par e-mail.
- **Étapes** :
  1. Créez une table d’audit :
     ```sql
     CREATE TABLE HighValueOrderLog (
         OrderID INT,
         TotalDue DECIMAL(18, 2),
         DetectedAt DATETIME
     );
     ```
  2. Créez un Job **HighValueAlert** :
     - **Étape 1 : Identifier les commandes** :
       ```sql
       INSERT INTO HighValueOrderLog (OrderID, TotalDue, DetectedAt)
       SELECT SalesOrderID, TotalDue, GETDATE()
       FROM AdventureWorks2022.Sales.SalesOrderHeader
       WHERE TotalDue > 50000;
       ```
     - **Étape 2 : Notifier via Database Mail** si des commandes sont détectées :
       ```sql
       IF EXISTS (SELECT 1 FROM HighValueOrderLog WHERE DetectedAt = CAST(GETDATE() AS DATE))
       BEGIN
           EXEC msdb.dbo.sp_send_dbmail
               @profile_name = 'PapercutProfile',
               @recipients = 'me780411@gmail.com',
               @subject = 'High Value Order Detected',
               @body = 'A sales order exceeding $50,000 has been logged.';
       END;
       ```

---

### **Exercice 13 : Job sécurisé avec Proxy pour exporter les logs SQL**
- **Objectif** : Utiliser un Proxy pour exécuter un Job exportant les logs SQL Server.
- **Étapes** :
  1. Configurez une Credential et associez-la à un Proxy :
     ```sql
     CREATE CREDENTIAL ProxyCredential 
     WITH IDENTITY = 'WindowsUserName', SECRET = 'Password123';
     EXEC sp_add_proxy @proxy_name = 'LogExportProxy', @credential_name = 'ProxyCredential';
     ```
  2. Créez un Job nommé **ExportSQLLogs** :
     - **Étape 1 : Collecter les logs** :
       ```sql
       SELECT * INTO SQLServerLogs
       FROM sys.dm_exec_requests;
       ```
     - **Étape 2 : Exporter les logs avec BCP** :
       ```powershell
       bcp "SELECT * FROM SQLServerLogs" queryout "C:\Logs\SQLServerLogs.csv" -c -T
       ```

---

### **Exercice 14 : Job multi-steps pour surveiller la fragmentation des index**
- **Objectif** : Détecter les index fragmentés et reconstruire les plus critiques.
- **Étapes** :
  1. Créez un Job **IndexFragmentationMonitor**.
  2. **Étape 1 : Identifier les index fragmentés** :
     - **Commande T-SQL** :
       ```sql
       SELECT OBJECT_NAME(object_id) AS TableName, 
              index_id, 
              avg_fragmentation_in_percent
       INTO FragmentedIndexes
       FROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'LIMITED')
       WHERE avg_fragmentation_in_percent > 30;
       ```
  3. **Étape 2 : Reconstruire les index** :
     - **Commande T-SQL** :
       ```sql
       DECLARE @TableName NVARCHAR(255), @IndexID INT;
       DECLARE CursorIndex CURSOR FOR
       SELECT TableName, index_id FROM FragmentedIndexes;
       OPEN CursorIndex;
       FETCH NEXT FROM CursorIndex INTO @TableName, @IndexID;
       WHILE @@FETCH_STATUS = 0
       BEGIN
           EXEC ('ALTER INDEX ALL ON ' + @TableName + ' REBUILD');
           FETCH NEXT FROM CursorIndex INTO @TableName, @IndexID;
       END;
       CLOSE CursorIndex;
       DEALLOCATE CursorIndex;
       ```

---

### **Exercice 15 : Surveillance de l’espace disque**
- **Objectif** : Détecter si l’espace disque est insuffisant et envoyer une notification.
- **Étapes** :
  1. Créez un Job **DiskSpaceMonitor**.
  2. **Étape 1 : Vérifiez l’espace disque** :
     ```sql
     SELECT DISTINCT volume_mount_point, available_bytes / (1024 * 1024) AS FreeSpaceMB
     INTO DiskSpaceStatus
     FROM sys.dm_os_volume_stats(DB_ID(), 1)
     WHERE available_bytes < 500 * 1024 * 1024; -- Moins de 500MB
     ```
  3. **Étape 2 : Envoyez une alerte** si la condition est remplie :
     ```sql
     IF EXISTS (SELECT 1 FROM DiskSpaceStatus)
     BEGIN
         EXEC msdb.dbo.sp_send_dbmail
             @profile_name = 'PapercutProfile',
             @recipients = 'me780411@gmail.com',
             @subject = 'Disk Space Alert',
             @body = 'The available disk space is below 500MB.';
     END;
     ```

---

### **Exercice 16 : Automatiser un ETL complet**
- **Objectif** : Extraire les ventes, les transformer en regroupant par produit, et charger dans une table.
- **Étapes** :
  1. Créez un Job nommé **ProductSalesETL**.
  2. **Étape 1 : Extraction des ventes** :
     ```sql
     SELECT * INTO TempSalesData
     FROM AdventureWorks2022.Sales.SalesOrderDetail;
     ```
  3. **Étape 2 : Transformation des données** :
     ```sql
     SELECT ProductID, SUM(LineTotal) AS TotalSales, COUNT(*) AS OrderCount
     INTO TransformedSales
     FROM TempSalesData
     GROUP BY ProductID;
     ```
  4. **Étape 3 : Chargement** :
     ```sql
     INSERT INTO AggregatedProductSales
     SELECT * FROM TransformedSales;
     ```

---
Merci pour votre retour ! Je vais revoir les exercices 17, 18, 19, et 20 pour **ajouter des détails concrets** et proposer des scénarios où les **résultats sont immédiatement observables**. Ces cas seront centrés sur des tâches **ETL** et des utilisations pertinentes et tangibles.

---

### **Exercice 17 : Simuler un échec de Job et vérifier l'alerte**
- **Objectif** : Simuler un échec intentionnel pour valider la configuration d’une alerte avec Database Mail.
- **Étapes** :
  1. **Configurer une alerte en cas d’échec** :
     - Créez une alerte dans **SQL Server Agent** :
       - **Nom** : `JobFailureAlert`.
       - **Type** : SQL Server Event Alert.
       - **ID d’erreur** : 50000 (Erreur utilisateur personnalisée).
     - Associez cette alerte à un **Operator** (par exemple : `AdminOperator`).

  2. **Créer un Job qui échoue volontairement** :
     - Créez un Job nommé **SimulateFailure**.
     - Ajoutez une étape avec une commande T-SQL incorrecte :
       ```sql
       SELECT * FROM NonExistentTable; -- Génère une erreur
       ```
  3. **Tester l’exécution** :
     - Lancez le Job manuellement.
     - Vérifiez que l’erreur est capturée dans **SQL Server Agent**.
     - Vérifiez que l’e-mail d’alerte est reçu dans **Papercut SMTP**.

  4. **Résultat attendu** :
     - Une alerte est envoyée avec l’erreur capturée :
       ```
       Job 'SimulateFailure' failed at Step 1 with error: Invalid object name 'NonExistentTable'.
       ```

---

### **Exercice 18 : Planifier une tâche ETL avec un Proxy**
- **Objectif** : Utiliser un Proxy pour automatiser un ETL consistant à extraire, transformer, et charger des données dans une base secondaire.
- **Étapes** :
  1. **Créer un Proxy sécurisé** :
     - Configurez une Credential et associez-la à un Proxy :
       ```sql
       CREATE CREDENTIAL ProxyCredential 
       WITH IDENTITY = 'WindowsUserName', SECRET = 'Password123';
       EXEC sp_add_proxy 
           @proxy_name = 'ETLProxy', 
           @credential_name = 'ProxyCredential';
       ```

  2. **Créer un Job multi-steps avec le Proxy** :
     - Nom : **AutomatedETL**.
     - Assurez-vous que le Proxy est assigné à chaque étape.

     **Étape 1 : Extraction des données** :
     ```sql
     SELECT * INTO TempSalesData
     FROM AdventureWorks2022.Sales.SalesOrderHeader
     WHERE OrderDate >= DATEADD(MONTH, -1, GETDATE());
     ```

     **Étape 2 : Transformation des données** :
     ```sql
     SELECT CustomerID, SUM(TotalDue) AS TotalSpent
     INTO TransformedSalesData
     FROM TempSalesData
     GROUP BY CustomerID;
     ```

     **Étape 3 : Chargement des données** :
     ```sql
     INSERT INTO ETL_Destination.dbo.AggregatedSales
     SELECT * FROM TransformedSalesData;
     ```

  3. **Planifier le Job** :
     - Associez un Schedule pour l’exécution quotidienne à 3h00.

  4. **Tester l’exécution** :
     - Lancez le Job manuellement.
     - Vérifiez les données chargées dans la table `ETL_Destination.dbo.AggregatedSales`.

  5. **Résultat attendu** :
     - Les données sont extraites, transformées, et chargées dans la base secondaire.

---

### **Exercice 19 : Auditer les permissions des utilisateurs et exporter dans un fichier**
- **Objectif** : Créer un Job pour auditer les permissions des utilisateurs et sauvegarder les résultats dans un fichier CSV.
- **Étapes** :
  1. **Créer un Job nommé `AuditUserPermissions`** :
     - **Étape 1 : Collecter les permissions utilisateurs** :
       ```sql
       SELECT dp.name AS UserName, 
              o.name AS ObjectName, 
              p.permission_name AS PermissionType
       INTO UserPermissionsAudit
       FROM sys.database_permissions p
       JOIN sys.objects o ON p.major_id = o.object_id
       JOIN sys.database_principals dp ON p.grantee_principal_id = dp.principal_id;
       ```

     - **Étape 2 : Exporter les données** dans un fichier CSV :
       ```powershell
       bcp "SELECT * FROM UserPermissionsAudit" queryout "C:\Exports\UserPermissions.csv" -c -T
       ```

  2. **Résultat attendu** :
     - Un fichier CSV est généré à l’emplacement `C:\Exports\UserPermissions.csv`.
     - Le contenu du fichier est immédiatement vérifiable dans Excel.

---

### **Exercice 20 : Détecter les transactions longues et les charger dans une table**
- **Objectif** : Identifier les transactions dépassant une durée donnée et charger les résultats dans une table d’audit.
- **Étapes** :
  1. **Créer une table d’audit des transactions longues** :
     ```sql
     CREATE TABLE LongRunningTransactionsAudit (
         TransactionID BIGINT,
         StartTime DATETIME,
         DurationMinutes INT
     );
     ```

  2. **Créer un Job nommé `MonitorLongTransactions`** :
     - **Étape 1 : Identifier les transactions longues** :
       ```sql
       INSERT INTO LongRunningTransactionsAudit (TransactionID, StartTime, DurationMinutes)
       SELECT transaction_id, start_time, DATEDIFF(MINUTE, start_time, GETDATE()) AS DurationMinutes
       FROM sys.dm_tran_active_transactions
       WHERE DATEDIFF(MINUTE, start_time, GETDATE()) > 10;
       ```

     - **Étape 2 : Envoyer une alerte si des transactions sont détectées** :
       ```sql
       IF EXISTS (SELECT 1 FROM LongRunningTransactionsAudit WHERE DurationMinutes > 10)
       BEGIN
           EXEC msdb.dbo.sp_send_dbmail
               @profile_name = 'PapercutProfile',
               @recipients = 'me780411@gmail.com',
               @subject = 'Long Running Transaction Alert',
               @body = 'One or more transactions have exceeded the allowed duration.';
       END;
       ```

  3. **Résultat attendu** :
     - Les transactions longues sont insérées dans la table `LongRunningTransactionsAudit`.
     - Une alerte est envoyée pour signaler les cas détectés.
    
   

